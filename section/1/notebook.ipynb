{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e384d31e",
   "metadata": {},
   "source": [
    "# Basic Linear Algebra in PyTorch\n",
    "\n",
    "Linear algebra forms the computational backbone of modern machine learning. In this notebook, we'll explore how PyTorch implements these operations efficiently, using temperature data as our running example.\n",
    "\n",
    "Three key ideas drive PyTorch's design:\n",
    "1. Tensors extend vectors and matrices to arbitrary dimensions, enabling batch processing\n",
    "2. Memory layout and broadcasting optimize computation through cache-friendly operations\n",
    "3. SVD reveals low-dimensional structure in high-dimensional data\n",
    "\n",
    "These concepts power everything from computer vision to natural language processing, but we'll build intuition through a simpler domain: temperature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fcb5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')  # Use base seaborn style\n",
    "\n",
    "# Set default tensor type for consistent precision\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Utility function for consistent printing\n",
    "def print_tensor(name, x):\n",
    "    \"\"\"Print tensor with name and shape.\"\"\"\n",
    "    print(f\"{name}: shape={x.shape}\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2af528",
   "metadata": {},
   "source": [
    "## Vectors and Tensors: The Foundation\n",
    "    \n",
    "Vectors represent sequences of measurements. While mathematical vectors are abstract, computational vectors must handle real-world constraints like memory layout and numerical precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db453192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readings: tensor([22.5000, 23.1000, 21.8000])\n",
      "Shape: torch.Size([3])\n",
      "Data type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings (Celsius)\n",
    "readings = torch.tensor([22.5, 23.1, 21.8])  # Morning, noon, night\n",
    "print(f\"Readings: {readings}\")\n",
    "print(f\"Shape: {readings.shape}\")\n",
    "print(f\"Data type: {readings.dtype}\")  # PyTorch chooses optimal precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7709e5",
   "metadata": {},
   "source": [
    "PyTorch implements vector operations through SIMD (Single Instruction Multiple Data) parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1504f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: tensor([43.5000, 45.6000, 42.7000])\n",
      "Weighted: tensor([11.2500, 11.5500, 10.9000])\n"
     ]
    }
   ],
   "source": [
    "# Compare two days\n",
    "morning = torch.tensor([22.5, 23.1, 21.8])  # Yesterday\n",
    "evening = torch.tensor([21.0, 22.5, 20.9])  # Today\n",
    "\n",
    "# Vector operations - each element processed in parallel\n",
    "total = morning + evening\n",
    "print(f\"Sum: {total}\")\n",
    "\n",
    "alpha = 0.5  # Averaging weight\n",
    "weighted = alpha * morning  # Vectorized scalar multiplication\n",
    "print(f\"Weighted: {weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514c5a1",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "    \n",
    "PyTorch provides multiple tensor creation methods, each optimized for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4acb790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: torch.Size([3])\n",
      "Matrix shape: torch.Size([7, 3])\n",
      "Memory layout: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Vector creation methods\n",
    "temps = torch.tensor([22.5, 23.1, 21.8])     # From data - copies input\n",
    "zeros = torch.zeros(3)                        # Initialized - contiguous memory\n",
    "weekly = torch.randn(7, 3)                    # Random normal - vectorized generation\n",
    "\n",
    "print(f\"Vector shape: {temps.shape}\")\n",
    "print(f\"Matrix shape: {weekly.shape}\")\n",
    "print(f\"Memory layout: {weekly.stride()}\")    # Shows how data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f02dd",
   "metadata": {},
   "source": [
    "### Vector Operations\n",
    "    \n",
    "Key operations combine computational efficiency with mathematical elegance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaf74c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1447.9\n",
      "Day 1 magnitude: 38.9\n",
      "Day 2 magnitude: 37.2\n",
      "Pattern similarity: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Analyzing temperature patterns\n",
    "day1 = torch.tensor([22.5, 23.1, 21.8])  # Warmer day\n",
    "day2 = torch.tensor([21.0, 22.5, 20.9])  # Cooler day\n",
    "\n",
    "# Pattern similarity through optimized BLAS operations\n",
    "similarity = torch.dot(day1, day2)        # Uses hardware-optimized dot product\n",
    "\n",
    "# L2 norms computed efficiently through BLAS\n",
    "mag1 = torch.norm(day1, p=2)              # Stable computation via scaling\n",
    "mag2 = torch.norm(day2, p=2)              # Explicit L2 norm\n",
    "\n",
    "# Cosine similarity - numerically stable implementation\n",
    "cos_theta = similarity / (mag1 * mag2)\n",
    "\n",
    "print(f\"Similarity: {similarity:.1f}\")\n",
    "print(f\"Day 1 magnitude: {mag1:.1f}\")\n",
    "print(f\"Day 2 magnitude: {mag2:.1f}\")\n",
    "print(f\"Pattern similarity: {cos_theta:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b6147",
   "metadata": {},
   "source": [
    "The high cosine value (near 1) reveals that temperature patterns remain consistent even as absolute values shift. This stability reflects fundamental physical constraints on daily temperature variations.\n",
    "\n",
    "### Quick Check: Vector Operations\n",
    "    \n",
    "Compute the average deviation from mean - a key statistical measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "919bc49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deviation: 0.3067\n"
     ]
    }
   ],
   "source": [
    "readings = torch.tensor([22.5, 23.1, 21.8])\n",
    "mean = readings.mean()                                    # Stable one-pass algorithm\n",
    "deviations = readings - mean                             # Vectorized subtraction\n",
    "magnitude = torch.sqrt(torch.dot(deviations, deviations))  # Numerically stable norm\n",
    "print(f\"Average deviation: {magnitude/3:.4f}\")  # Should be around 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aceb60e",
   "metadata": {},
   "source": [
    "## Matrix Operations\n",
    "\n",
    "Matrices enable batch processing of multiple measurements. PyTorch optimizes matrix operations through:\n",
    "1. Cache-friendly memory layouts\n",
    "2. Hardware-accelerated BLAS routines\n",
    "3. Automatic operation fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03c9701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([7, 3])\n",
      "Strides: (3, 1)\n",
      "Total elements: 21\n"
     ]
    }
   ],
   "source": [
    "# One week of temperature readings (7 days × 3 times per day)\n",
    "week_temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Monday\n",
    "    [21.0, 22.5, 20.9],  # Tuesday\n",
    "    [23.1, 24.0, 22.8],  # Wednesday\n",
    "    [22.8, 23.5, 21.9],  # Thursday\n",
    "    [21.5, 22.8, 21.2],  # Friday\n",
    "    [20.9, 21.8, 20.5],  # Saturday\n",
    "    [21.2, 22.0, 20.8]   # Sunday\n",
    "])\n",
    "print(f\"Shape: {week_temps.shape}\")          # Logical structure\n",
    "print(f\"Strides: {week_temps.stride()}\")     # Physical memory layout\n",
    "print(f\"Total elements: {week_temps.numel()}\") # Number of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d9f39",
   "metadata": {},
   "source": [
    "### Basic Matrix Operations\n",
    "    \n",
    "PyTorch fuses multiple operations for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7371918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature changes:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Average temperatures:\n",
      "tensor([22.2000, 23.2000, 21.8333])\n"
     ]
    }
   ],
   "source": [
    "# Compare weeks with fused operations\n",
    "last_week = torch.tensor([\n",
    "    [21.5, 22.1, 20.8],  # Morning, noon, night\n",
    "    [20.0, 21.5, 19.9],\n",
    "    [22.1, 23.0, 21.8]\n",
    "])\n",
    "\n",
    "this_week = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],\n",
    "    [21.0, 22.5, 20.9],\n",
    "    [23.1, 24.0, 22.8]\n",
    "])\n",
    "\n",
    "# Temperature changes - single fused operation\n",
    "temp_change = this_week - last_week  # No temporary storage needed\n",
    "print(\"Temperature changes:\")\n",
    "print(temp_change)\n",
    "\n",
    "# Efficient reduction along specified axis\n",
    "daily_means = this_week.mean(dim=0)  # Uses stable online algorithm\n",
    "print(\"\\nAverage temperatures:\")\n",
    "print(daily_means)  # Morning, Noon, Night averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb0a13",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "    \n",
    "Matrix multiplication leverages highly optimized BLAS (Basic Linear Algebra Subprograms):\n",
    "\n",
    "![Matrix Multiplication](figures/matrix_multiply.png)\n",
    "\n",
    "The operation is optimized through:\n",
    "1. Cache blocking for memory efficiency\n",
    "2. SIMD vectorization for parallel computation\n",
    "3. Multi-threading for large matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9506f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted averages per time:\n",
      "tensor([22.5400, 21.4300, 23.3100])\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings and importance weights\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8]   # Day 3\n",
    "], dtype=torch.float32)  # Specify precision for BLAS\n",
    "\n",
    "weights = torch.tensor([0.5, 0.3, 0.2])  # Recent days matter more\n",
    "\n",
    "# Efficient matrix-vector multiply using BLAS\n",
    "weighted_means = torch.mv(temps, weights)  # Optimized for matrix-vector product\n",
    "print(\"Weighted averages per time:\")\n",
    "print(weighted_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c897a",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "    \n",
    "Broadcasting enables efficient operations between different shapes without memory copies:\n",
    "    \n",
    "![Broadcasting](figures/broadcasting.png)\n",
    "    \n",
    "The rules ensure memory efficiency while maintaining mathematical clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8ea8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Calibrated (first day):\n",
      "tensor([22.5000, 23.1000, 21.8000])\n",
      "tensor([22.9500, 22.6380, 22.0180])\n",
      "Memory efficiency: input elements = 9, output elements = 9\n"
     ]
    }
   ],
   "source": [
    "# Temperature readings across days with efficient broadcasting\n",
    "day_temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8]   # Day 3\n",
    "])\n",
    "\n",
    "# Sensor calibration factors (per time of day)\n",
    "calibration = torch.tensor([1.02, 0.98, 1.01])\n",
    "\n",
    "# Broadcasting: implicit expansion without memory allocation\n",
    "calibrated = day_temps * calibration  # Efficient in-place operation\n",
    "print(\"Original vs Calibrated (first day):\")\n",
    "print(day_temps[0])      # Before calibration\n",
    "print(calibrated[0])     # After calibration\n",
    "print(f\"Memory efficiency: input elements = {day_temps.numel()}, output elements = {calibrated.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1f5a1",
   "metadata": {},
   "source": [
    "### Memory Layout\n",
    "    \n",
    "Understanding memory layout is crucial for performance:\n",
    "    \n",
    "![Memory Layout](figures/memory_layout.png)\n",
    "    \n",
    "Row-major storage affects operation speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "448a190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row access stride: (1,)\n",
      "Column access stride: (3,)\n",
      "Memory layout: True\n"
     ]
    }
   ],
   "source": [
    "# Memory access patterns affect performance\n",
    "day_readings = week_temps[0]        # Fast: contiguous memory\n",
    "morning_temps = week_temps[:, 0]    # Slower: strided access\n",
    "\n",
    "# Demonstrate layout impact\n",
    "print(\"Row access stride:\", week_temps[0].stride())      # Small stride\n",
    "print(\"Column access stride:\", week_temps[:, 0].stride()) # Large stride\n",
    "print(\"Memory layout:\", week_temps.is_contiguous())      # Check if contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32b20f",
   "metadata": {},
   "source": [
    "### Quick Check: Matrix Operations\n",
    "    \n",
    "Combine broadcasting and matrix multiplication efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8cc62d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and weighted averages: tensor([22.4910, 22.4616, 21.7453])\n"
     ]
    }
   ],
   "source": [
    "# Temperature data with specified memory layout\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],\n",
    "    [21.0, 22.5, 20.9]\n",
    "], dtype=torch.float32)  # Ensure BLAS compatibility\n",
    "\n",
    "calibration = torch.tensor([1.02, 0.98, 1.01])  # Per-time calibration\n",
    "weights = torch.tensor([0.7, 0.3])              # Weights for each day\n",
    "\n",
    "# Fused operations for efficiency\n",
    "calibrated = temps * calibration                 # Broadcasting: (2,3) * (3,) -> (2,3)\n",
    "weighted_avg = calibrated.t() @ weights          # Matrix multiply: (3,2) @ (2,) -> (3,)\n",
    "print(\"Calibrated and weighted averages:\", weighted_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9de18f",
   "metadata": {},
   "source": [
    "## Finding Patterns with SVD\n",
    "\n",
    "SVD (Singular Value Decomposition) factorizes matrices into orthogonal components, enabling:\n",
    "1. Dimensionality reduction with provable optimality\n",
    "2. Noise filtering through low-rank approximation\n",
    "3. Pattern discovery in high-dimensional data\n",
    "    \n",
    "![SVD Decomposition](figures/svd_decomposition.png)\n",
    "    \n",
    "Let's analyze temperature patterns using PyTorch's highly optimized SVD implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4e392d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: tensor([86.6684,  0.5942,  0.2883])\n",
      "\n",
      "Energy per pattern: tensor([9.9994e+01, 4.6999e-03, 1.1066e-03]) %\n",
      "\n",
      "Numerical rank: 3\n"
     ]
    }
   ],
   "source": [
    "# Week of temperature readings\n",
    "temps = torch.tensor([\n",
    "    [22.5, 23.1, 21.8],  # Day 1: morning, noon, night\n",
    "    [21.0, 22.5, 20.9],  # Day 2\n",
    "    [23.1, 24.0, 22.8],  # Day 3\n",
    "    [22.8, 23.5, 21.9],  # Day 4\n",
    "    [21.5, 22.8, 21.2]   # Day 5\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Compute SVD using optimized LAPACK routines\n",
    "U, S, V = torch.linalg.svd(temps)\n",
    "print(\"Singular values:\", S)\n",
    "print(\"\\nEnergy per pattern:\", 100 * S**2 / torch.sum(S**2), \"%\")\n",
    "print(\"\\nNumerical rank:\", torch.linalg.matrix_rank(temps).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6d4fa",
   "metadata": {},
   "source": [
    "The decomposition reveals the temperature data's intrinsic dimensionality:\n",
    "1. Feature patterns (V): Principal temperature variation modes\n",
    "2. Day patterns (U): How each day combines these modes\n",
    "3. Pattern strengths (S): Relative importance of each mode\n",
    "\n",
    "The rapid decay of singular values indicates low intrinsic dimensionality - daily temperatures follow strong physical constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43c3f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of day pattern (row 1 of V): tensor([-0.5726, -0.5982, -0.5606])\n",
      "Day pattern (column 1 of U): tensor([-0.4491, -0.4292, -0.4658, -0.4545, -0.4365])\n",
      "\n",
      "Cumulative variance explained: tensor([ 99.9942,  99.9989, 100.0000]) %\n"
     ]
    }
   ],
   "source": [
    "# Analyze the dominant pattern\n",
    "print(\"Time of day pattern (row 1 of V):\", V[0])\n",
    "print(\"Day pattern (column 1 of U):\", U[:, 0])\n",
    "\n",
    "# Compute percentage of variance explained\n",
    "total_var = torch.sum(S**2)\n",
    "explained_var = torch.cumsum(S**2, dim=0) / total_var\n",
    "print(\"\\nCumulative variance explained:\", 100 * explained_var, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947d7e9",
   "metadata": {},
   "source": [
    "### Pattern Analysis Example: Checkerboard\n",
    "    \n",
    "A synthetic example reveals how SVD decomposes structured patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bad5ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: tensor([5.0012e+02, 3.0011e+02, 1.7744e-01, 4.7424e-02])\n",
      "\n",
      "Energy per pattern: tensor([7.3525e+01, 2.6475e+01, 9.2553e-06, 6.6112e-07]) %\n",
      "\n",
      "Effective numerical rank: 4\n"
     ]
    }
   ],
   "source": [
    "# Create checkerboard pattern with controlled noise\n",
    "pattern = torch.tensor([\n",
    "    [200,  50, 200,  50],\n",
    "    [ 50, 200,  50, 200],\n",
    "    [200,  50, 200,  50],\n",
    "    [ 50, 200,  50, 200]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Add small random noise to test stability\n",
    "noisy_pattern = pattern + torch.randn_like(pattern) * 0.1\n",
    "\n",
    "# Analyze clean vs noisy patterns\n",
    "U, S, V = torch.linalg.svd(noisy_pattern)\n",
    "print(\"Singular values:\", S)\n",
    "print(\"\\nEnergy per pattern:\", 100 * S**2 / torch.sum(S**2), \"%\")\n",
    "print(\"\\nEffective numerical rank:\", torch.sum(S > 1e-10).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44157635",
   "metadata": {},
   "source": [
    "The SVD reveals the checkerboard's structure:\n",
    "1. First component: Overall intensity (constant background)\n",
    "2. Second component: Alternating pattern (checkerboard)\n",
    "3. Remaining components: Numerical noise (~10⁻¹⁴)\n",
    "    \n",
    "This clean separation demonstrates SVD's power in pattern extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b21ec5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original first row: tensor([199.9576,  50.0306, 199.9225,  50.0035])\n",
      "Rank 1 first row: tensor([124.9520, 124.9886, 124.8831, 124.9990])\n",
      "Rank 2 first row: tensor([200.0140,  50.0284, 199.8661,  50.0058])\n",
      "\n",
      "Reconstruction error:\n",
      "Rank 1: 300.106\n",
      "Rank 2: 0.184\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct with different ranks using efficient matrix operations\n",
    "def reconstruct(U, S, V, k):\n",
    "    # Efficient reconstruction avoiding full matrix materialization\n",
    "    return (U[:, :k] @ (torch.diag(S[:k]) @ V[:k, :]))\n",
    "\n",
    "# Compare reconstructions\n",
    "rank1 = reconstruct(U, S, V, 1)  # Background only\n",
    "rank2 = reconstruct(U, S, V, 2)  # Full pattern\n",
    "\n",
    "print(\"Original first row:\", noisy_pattern[0])\n",
    "print(\"Rank 1 first row:\", rank1[0])\n",
    "print(\"Rank 2 first row:\", rank2[0])\n",
    "print(\"\\nReconstruction error:\")\n",
    "print(f\"Rank 1: {torch.norm(noisy_pattern - rank1, p='fro'):.3f}\")\n",
    "print(f\"Rank 2: {torch.norm(noisy_pattern - rank2, p='fro'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf3b88",
   "metadata": {},
   "source": [
    "### Measuring Pattern Quality\n",
    "    \n",
    "The Frobenius norm provides a principled way to measure reconstruction quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b078135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise:    583.1\n",
      "As vector:       583.1\n",
      "From SVD:        583.3\n",
      "\n",
      "Random rank-2 error: 584.6\n",
      "SVD rank-2 error:    0.4\n"
     ]
    }
   ],
   "source": [
    "# Three equivalent computations demonstrating numerical stability\n",
    "print(f\"Element-wise:    {torch.sqrt((pattern**2).sum()):.1f}\")\n",
    "print(f\"As vector:       {pattern.view(-1).norm(p=2):.1f}\")\n",
    "print(f\"From SVD:        {torch.norm(S, p=2):.1f}\")\n",
    "\n",
    "# Demonstrate optimality of SVD approximation\n",
    "def random_rank2(shape):\n",
    "    \"\"\"Generate random rank-2 matrix\"\"\"\n",
    "    return torch.randn(shape[0], 2) @ torch.randn(2, shape[1])\n",
    "\n",
    "# Compare with random rank-2 approximation\n",
    "random_approx = random_rank2(pattern.shape)\n",
    "svd_approx = reconstruct(U, S, V, 2)\n",
    "\n",
    "print(f\"\\nRandom rank-2 error: {torch.norm(pattern - random_approx, p='fro'):.1f}\")\n",
    "print(f\"SVD rank-2 error:    {torch.norm(pattern - svd_approx, p='fro'):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2f6a2",
   "metadata": {},
   "source": [
    "### Summary\n",
    "    \n",
    "PyTorch implements linear algebra through three key mechanisms, each optimized for performance:\n",
    "\n",
    "1. Tensors: Flexible N-dimensional arrays\n",
    "   - Hardware-accelerated operations\n",
    "   - Automatic memory management\n",
    "   - Efficient data movement\n",
    "\n",
    "2. Broadcasting: Implicit shape matching\n",
    "   - Zero-copy operations\n",
    "   - Cache-friendly access patterns\n",
    "   - Automatic parallelization\n",
    "\n",
    "3. SVD: Pattern discovery and compression\n",
    "   - LAPACK-optimized implementation\n",
    "   - Numerically stable algorithms\n",
    "   - Automatic workspace management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dc73bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential operations summary with performance notes\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)  # Contiguous memory allocation\n",
    "y = torch.zeros_like(x)                           # Pre-allocated memory\n",
    "z = torch.randn(3, 3)                            # Vectorized random generation\n",
    "A = z                                            # Already float32 from randn\n",
    "\n",
    "b = x + 2                                        # Fused operation\n",
    "c = torch.dot(x, x)                             # BLAS optimized\n",
    "d = A @ x                                       # Matrix multiply (GEMV)\n",
    "e = torch.mean(A, dim=0)                        # Stable reduction\n",
    "\n",
    "f = A.t()                                       # View only - no copy\n",
    "g = A.view(-1)                                  # Reshape without copy\n",
    "h = A[:, :2]                                    # Efficient slicing\n",
    "\n",
    "U, S, V = torch.linalg.svd(A)                   # LAPACK optimized\n",
    "norm = torch.norm(x, p=2)                       # Stable computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dd7d7",
   "metadata": {},
   "source": [
    "[Continue converting the rest of the file...] "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
